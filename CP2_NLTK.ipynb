{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('training_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_text = train_data['text']\n",
    "train_label = train_data['stars']\n",
    "test_data_text = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns that used to find or/and replace particular chars or words\n",
    "# to find chars that are not a letter, a blank or a quotation\n",
    "pat_letter = re.compile(r'[^a-zA-Z \\']+')\n",
    "# to find the 's following the pronouns. re.I is refers to ignore case\n",
    "pat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n",
    "# to find the 's following the letters\n",
    "pat_s = re.compile(\"(?<=[a-zA-Z])\\'s\")\n",
    "# to find the ' following the words ending by s\n",
    "pat_s2 = re.compile(\"(?<=s)\\'s?\")\n",
    "# to find the abbreviation of not\n",
    "pat_not = re.compile(\"(?<=[a-zA-Z])n\\'t\")\n",
    "# to find cannot\n",
    "pat_cannot = re.compile(\"cannot\")\n",
    "# to find the abbreviation of would\n",
    "pat_would = re.compile(\"(?<=[a-zA-Z])\\'d\")\n",
    "# to find the abbreviation of will\n",
    "pat_will = re.compile(\"(?<=[a-zA-Z])\\'ll\")\n",
    "# to find the abbreviation of am\n",
    "pat_am = re.compile(\"(?<=[I|i])\\'m\")\n",
    "# to find the abbreviation of are\n",
    "pat_are = re.compile(\"(?<=[a-zA-Z])\\'re\")\n",
    "# to find the abbreviation of have\n",
    "pat_ve = re.compile(\"(?<=[a-zA-Z])\\'ve\")\n",
    "# to find the abbreviation of a.m./p.m.\n",
    "pat_AMPM = re.compile(\"a m|p m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Regular Expression and Stop Words to clean the text\n",
    "with_negative_words = 0 # Set 1 if apply LSTM; set 0 if apply Xgboost Regressor\n",
    "\n",
    "with open('common_english_words.txt') as f:\n",
    "    content = f.readlines()\n",
    "my_stop_words = content[0].split(',')\n",
    "stop_words = set(ENGLISH_STOP_WORDS.union(my_stop_words))\n",
    "stop_words.add('ca')\n",
    "stop_words.remove('cry')\n",
    "stop_words.remove('dear')\n",
    "stop_words.remove('interest')\n",
    "stop_words.remove('like')\n",
    "stop_words.remove('never')\n",
    "stop_words.remove('please')\n",
    "stop_words.remove('serious')\n",
    "stop_words.remove('top')\n",
    "stop_words.remove('well')\n",
    "if with_negative_words==1:\n",
    "    stop_words.remove('against')\n",
    "    stop_words.remove('except')\n",
    "    stop_words.remove('neither')\n",
    "    stop_words.remove('no')\n",
    "    stop_words.remove('nor')\n",
    "    stop_words.remove('not')\n",
    "    stop_words.remove('none')\n",
    "stop_words = frozenset(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to replace abbreviation\n",
    "def replace_abbreviations(text):\n",
    "    new_text = text\n",
    "    new_text = pat_letter.sub(' ', text).strip().lower()\n",
    "    new_text = pat_is.sub(r\"\\1 is\", new_text)\n",
    "    new_text = pat_s.sub(\"\", new_text)\n",
    "    new_text = pat_s2.sub(\"\", new_text)\n",
    "    new_text = pat_not.sub(\" not\", new_text)\n",
    "    new_text = pat_cannot.sub(\" can not\", new_text)\n",
    "    new_text = pat_would.sub(\" would\", new_text)\n",
    "    new_text = pat_will.sub(\" will\", new_text)\n",
    "    new_text = pat_am.sub(\" am\", new_text)\n",
    "    new_text = pat_are.sub(\" are\", new_text)\n",
    "    new_text = pat_ve.sub(\" have\", new_text)\n",
    "    new_text = pat_AMPM.sub(\"\", new_text)\n",
    "    new_text = new_text.replace('\\'', ' ')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to get the word's pos\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to lemmatize each words\n",
    "def lemmatize_word(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word:\n",
    "            tag = nltk.pos_tag(word_tokenize(word)) # tag is like [('bigger', 'JJR')]\n",
    "            pos = get_wordnet_pos(tag[0][1])\n",
    "            if pos:\n",
    "                lemmatized_word = lmtzr.lemmatize(word, pos)\n",
    "                new_words.append(lemmatized_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to clean the stop words\n",
    "def clean_stop_words(words,stops):\n",
    "    new_words = [w for w in words if not w in stops]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the functions above to preprocess the text data\n",
    "def preprocess_text(text,stops):\n",
    "    words = clean_stop_words(lemmatize_word(replace_abbreviations(text).split()),stops)\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].map(lambda x: preprocess_text(x,stop_words))\n",
    "test_data['text'] = test_data['text'].map(lambda x: preprocess_text(x,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avoid cost awful service slow hell disposable plasticware plate add landfill insult heinous food despite tell rush catch plane multiple reassurance food arrive plenty time lollygagged end pay seriously piss hungry food blech\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i=5\n",
    "print(train_data.loc[i,'text'])\n",
    "print(train_data.loc[i,'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_negative_words==1:\n",
    "    train_data.to_csv('train_after_nltk_with_negative.csv')\n",
    "    test_data.to_csv('test_after_nltk_with_negative.csv')\n",
    "else:\n",
    "    train_data.to_csv('train_after_nltk_without_negative.csv')\n",
    "    test_data.to_csv('test_after_nltk_without_negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
